{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SIN.06023 L'Apprentissage Automatique\n",
    "\n",
    "### Semestre de printemps 2025\n",
    "\n",
    "### Matthew Vowels\n",
    "(PhD Eng., PhD Appl. Math, MS, MSc, BMus (hons))\n",
    "\n",
    "Slides available at www.github.com/matthewvowels1/SIN06023\n",
    "\n",
    "<img src=\"logo.png\" alt=\"drawing\" width=\"600\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calendrier des cours\n",
    "\n",
    "Semaine 1: Revue des concepts clés de l'algèbre linéaire, de la théorie des probabilités et de l'optimisation\n",
    "\n",
    "Semaine 2: Algorithmes d'apprentissage, capacité et biais-variance, et 'maximum likelihood estimation'\n",
    "\n",
    "Semaine 3: Répartition et validation train/test, k-fold, optimisation des hyperparamètres\n",
    "\n",
    "Semaine 4: Apprentissage supervisé et non-supervisé\n",
    "\n",
    "Semaine 5: Machines vectorielles de support\n",
    "\n",
    "Semaine 6: Arbres de décision et forêts aléatoires\n",
    "\n",
    "Semaine 7: Réseaux de neurone: Rétropropagation, règle de chaîne et optimisation stochastique\n",
    "\n",
    "Semaine 8: Réseaux de neurones\n",
    "\n",
    "Semaine 9: Réseaux convolutifs\n",
    "\n",
    "**Semaine 10: Biais et équité dans l'apprentissage automatique**\n",
    "\n",
    "Semaine 11: Apprentissage automatique explicable (importances, SHAP, LIME) et calibration\n",
    "\n",
    "Semaine 12 : Récapitulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTK34uxMjMYL",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Semaine 10: Biais et équité dans l'apprentissage automatique\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Comme toujours, vous devez accéder au contenu du notebook Jupyter depuis le référentiel et l'ouvrir dans Google Colab (ou localement si vous le souhaitez).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Resources\n",
    "\n",
    "\n",
    "Solon Barocas, Moritz Hardt, Arvind Narayanan. Fairness and Machine Learning: Limitations and Opportunities\n",
    "https://fairmlbook.org\n",
    "\n",
    "Meredith Broussard. Artificial Unintelligence: How Computers Misunderstand the World\n",
    "\n",
    "Virginia Eubanks. Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor.\n",
    "\n",
    "Keynotes/Tutoriels par Joy Boulamwini, Kate Crawford, Meredith Whitaker et d'autres.\n",
    "\n",
    "\n",
    "Le contenu de cette conférence a été inspiré par le discours de Moritz Hardt sur l'équité ici :\n",
    "https://www.youtube.com/watch?v=Igq_S_7IfOU \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Citation motivante\n",
    "\n",
    "*'...The employment of new technologies that reflect and reproduce existing inequities but that are promoted and perceived as more objective or progressive than the discriminatory systems of a previous era.'* \n",
    "\n",
    "**Ruha Benjamin**\n",
    "\n",
    "*'... L'emploi de nouvelles technologies qui reflètent et reproduisent les inégalités existantes mais qui sont promues et perçues comme plus objectives ou progressistes que les systèmes discriminatoires d'une époque précédente.'*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<img src=\"bias1.png\" alt=\"drawing\" width=\"900\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<img src=\"bias2.png\" alt=\"drawing\" width=\"900\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<img src=\"bias3.png\" alt=\"drawing\" width=\"900\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<img src=\"bias4.png\" alt=\"drawing\" width=\"900\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "La loi essaie de suivre les progrès...\n",
    "\n",
    "- Règlement général sur la protection des données (RGPD) : article 22 : accorde aux individus le droit de ne pas être soumis à des décisions basées uniquement sur une prise de décision automatisée, ce qui implique un besoin de transparence et d'explicabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Il existe une interprétation du « droit à l’explication »"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- La loi évolue et évolue en réponse aux éventuels problèmes exacerbés par l’IA et l'apprentissage automatique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Qu'est-ce qu'un algorithme **équitable** ?\n",
    "\n",
    "\n",
    "### Comme nous le verrons, la notion d’équité dépend du contexte et de la perspective éthique adoptée.\n",
    "\n",
    "\n",
    "### Nous voulons éviter les algorithmes qui augmentent ou perpétuent la discrimination et les inégalités.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Alors, qu’est-ce qu’une *discrimination injustifiée* dans l’apprentissage automatique ?\n",
    "\n",
    "Deux types :\n",
    "\n",
    "- Non-pertinence pratique (orientation sexuelle dans les décisions d'emploi ou les recommandations d'emploi)\n",
    "\n",
    "- Non-pertinence morale – plus compliquée (statut de handicap ou grossesse dans les décisions d'embauche et impact, par exemple, sur le coût pour l'employeur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notez également qu'il existe diverses fonctionnalités sensibles (aux États-Unis par exemple, ce sont des classes protégées) :\n",
    "\n",
    "- Race\n",
    "- Genre\n",
    "- Grossesse\n",
    "- Religion\n",
    "- Statut familial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Anciennes cartes « ligne rouge » permettant aux agents de prêt de décider où prêter et où ne pas prêter - ce n'est plus légal aux États-Unis\n",
    "\n",
    "<img src=\"redline.png\" alt=\"drawing\" width=\"800\" align=\"center\"/>\n",
    "https://www.youtube.com/watch?v=Igq_S_7IfOU Moritz Hardt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Concevez un système capable d'évaluer s'il vaut la peine de livrer le jour même dans une région particulière ou non.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$${f}: \\mathbf{X} \\rightarrow y$$\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"prediction_task.png\" alt=\"drawing\" width=\"900\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exemple du monde réel...\n",
    "\n",
    "Couverture de livraison Amazon le jour même\n",
    "\n",
    "<img src=\"amazonMap.png\" alt=\"drawing\" width=\"1000\" align=\"center\"/>\n",
    "https://www.bloomberg.com/graphics/2016-amazon-same-day/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  L’utilisation (peut-être bien intentionnée) des algorithmes sans prise de conscience des problèmes conduit à des problèmes d’iniquité.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Des questions:\n",
    "\n",
    "(1) Est-il préférable de supprimer la variable race du problème de prédiction\n",
    "\n",
    "(2) Plus largement, ne devrait-on jamais considérer la race comme une variable ?\n",
    "\n",
    "\n",
    "<img src=\"prediction_task_mod.png\" alt=\"drawing\" width=\"900\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "la race est codée dans les autres variables (une variable latente), peut-être via des mécanismes sociétaux inéquitables que nous ne voulons pas perpétuer\n",
    "\n",
    "\n",
    "Donc l'option (1) ne fonctionnera pas...\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"prediction_task_latent.png\" alt=\"drawing\" width=\"900\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "De plus, si nous ne collectons pas la race, nous n’avons aucune possibilité d’évaluer si notre modèle prend indirectement des décisions en l’utilisant.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"prediction_task_latent.png\" alt=\"drawing\" width=\"900\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Donc « Nous n’utilisons pas cette variable » n’est pas un argument.\n",
    "\n",
    "### In english one might say 'there is no fairness without awareness'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alors, comment pouvons-nous évaluer et tester l’équité ?\n",
    "\n",
    "- Critères d'équité dans la classification\n",
    "\n",
    "- Modélisation causale et explicabilité  (la semaine prochaine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Les deux critères populaires\n",
    "\n",
    "- Taux positif égal:  un nombre égal de cas « acceptés » dans tous les groupes\n",
    "- Égalisation des taux d’erreur: nombre de faux positifs et de négatifs égal dans tous les groupes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### 1 **Taux positif égal**\n",
    "\n",
    "Pour le decision $\\hat{Y}$, l'appartenance au groupe $G$:\n",
    "\n",
    "\n",
    "$$p(\\hat{Y}=1 | G=a) = p(\\hat{Y}=1 | G=b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "i.e. s'assurer que le taux d'acceptation est égal dans tous les groupes, et donc que $\\hat{Y} \\perp\\!\\!\\!\\!\\perp G $\n",
    "\n",
    "Ex.: Zemel et al. (2015) https://proceedings.mlr.press/v28/zemel13.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "\n",
    "def calculate_equal_positive_rate(predictions: np.ndarray, groups: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the equal positive rate criterion for a decision classifier,\n",
    "    considering only the group membership.\n",
    "\n",
    "    :param predictions: A NumPy array containing binary predictions from the classifier.\n",
    "    :param groups: A NumPy array indicating the group membership of each prediction.\n",
    "    :return: A dictionary with the positive rate for each group.\n",
    "    \"\"\"\n",
    "    unique_groups = np.unique(groups)\n",
    "    positive_rates: Dict[str, float] = {}\n",
    "\n",
    "    for group in unique_groups:\n",
    "        # Mask for the current group\n",
    "        mask = (groups == group)\n",
    "\n",
    "        # Calculate positive rate for the group\n",
    "        positive_rate = predictions[mask].mean()\n",
    "        positive_rates[group] = positive_rate\n",
    "\n",
    "    return positive_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "N = 100\n",
    "\n",
    "# Générer quelques exemples de données\n",
    "y_true = np.random.randint(0, 2, size=N)\n",
    "\n",
    "groups = np.random.choice(['A', 'B'], size=N)\n",
    "print('Example group assignment:',groups[:5])\n",
    "\n",
    "y_pred = np.random.randint(0, 2, size=N)  #  Au lieu de cela, écoutez vos prédictions réelles\n",
    "print('Example predictions:', predictions[:5])\n",
    "\n",
    "result = calculate_equal_positive_rate(y_pred, groups)  # nous n'utilisons pas les vraies étiquettes\n",
    "print('Per-group positive rate:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### 1 **Taux positif égal** : faiblesses !\n",
    "\n",
    "\n",
    "On peut avoir un nombre élevé de vrais positifs dans un groupe et un nombre élevé de faux positifs dans un autre, et remplir le critère de taux de positivité égale.\n",
    "\n",
    "Il existe des solutions dégénérées qui satisfont au critère."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### 2 **Égalisation des taux d’erreur**\n",
    "\n",
    "\n",
    "Assurer un taux de faux positifs égal:\n",
    "\n",
    "$$p(\\hat{Y}=1|Y=0, G=a) = p(\\hat{Y}=1|Y=0, G=b)$$\n",
    "\n",
    "\n",
    "et garantir un taux de faux négatifs égal:\n",
    "\n",
    "$$p(\\hat{Y}=0|Y=1, G=a) = p(\\hat{Y}=0|Y=1, G=b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "et donc que $\\hat{Y} \\perp\\!\\!\\!\\!\\perp G | Y $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_false_rates(y_true: np.ndarray, y_pred: np.ndarray, groups: np.ndarray) -> Tuple[Dict[str, float], Dict[str, float]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the false positive and false negative rates for each group.\n",
    "\n",
    "    :param y_true: A NumPy array containing the true labels.\n",
    "    :param y_pred: A NumPy array containing the predicted labels.\n",
    "    :param groups: A NumPy array indicating the group membership of each label.\n",
    "    :return: A dictionary with the false positive and false negative rates for each group.\n",
    "    \"\"\"\n",
    "    unique_groups = np.unique(groups)\n",
    "    fp_rates: Dict[str, float] = {}\n",
    "    fn_rates: Dict[str, float] = {}\n",
    "\n",
    "    for group in unique_groups:\n",
    "        mask = (groups == group)\n",
    "\n",
    "        # False positives: where true label is 0 but predicted label is 1\n",
    "        false_positives = np.sum((y_true[mask] == 0) & (y_pred[mask] == 1))\n",
    "\n",
    "        # False negatives: where true label is 1 but predicted label is 0\n",
    "        false_negatives = np.sum((y_true[mask] == 1) & (y_pred[mask] == 0))\n",
    "\n",
    "        # Total negatives and positives \n",
    "        total_negatives = np.sum(y_true[mask] == 0)\n",
    "        total_positives = np.sum(y_true[mask] == 1)\n",
    "\n",
    "        fp_rate = false_positives / total_negatives if total_negatives > 0 else 0\n",
    "        fn_rate = false_negatives / total_positives if total_positives > 0 else 0\n",
    "\n",
    "        fp_rates[group] = fp_rate\n",
    "        fn_rates[group] = fn_rate\n",
    "\n",
    "    return fp_rates, fn_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fp_rates, fn_rates = calculate_false_rates(y_true, y_pred, groups)\n",
    "print('False positive rates:', fp_rates)\n",
    "print('False negative rates:', fn_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### 2 **Égalisation des taux d’erreur** : faiblesses !\n",
    "\n",
    "Ce critère ne peut être évalué qu'<après> avoir les vraies données - c'est un critère 'post-hoc'. \n",
    "\n",
    "Une telle évaluation peut arriver « trop tard ».\n",
    "\n",
    "\n",
    "Parfois, nous ne pouvons jamais l'évaluer... (par exemple, l'embauche)\n",
    "\n",
    "\n",
    "Cela ne nous aide pas non plus à éviter les solutions dégénérées..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### 3 **Calibration** \n",
    "\n",
    "Pour une fonction de score $R(\\mathbf{X})$ qui pourrait simplement être une estimation de $\\mathbb{E}[Y|\\mathbf{X}]$, un score $R$ est calibré si :\n",
    "\n",
    "$$p(Y=1 | R=r) = r$$\n",
    "\n",
    "Nous pouvons calibrer par groupe :\n",
    "\n",
    "$$p(Y=y | R=r, G=a) = r$$\n",
    "\n",
    "Nous voulons donc que notre score se comporte comme une probabilité au sein de chaque groupe et signifie donc ce qu'il est censé signifier.\n",
    "\n",
    "Cela découle de $Y \\perp\\!\\!\\!\\!\\perp G | R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### 3 **Calibration** \n",
    "\n",
    "En mots : si mon objectif est de prédire $Y$, après avoir observé le score $R$ il n'est pas nécessaire de connaître l'appartenance au groupe. Un r = 0,8 signifie 0,8 quel que soit le groupe.\n",
    "\n",
    "Si un modèle est calibré, cela signifie que le score reflète fidèlement la probabilité du véritable résultat $Y$, quelle que soit l'appartenance au groupe $G=a$ ou $G=b$ ou autre. Il garantit la fiabilité et la « fiabilité » du classificateur entre les groupes.\n",
    "\n",
    "Par exemple. r = 0,8 signifie un taux d'insuffisance cardiaque de 80 % en moyenne par rapport aux personnes qui reçoivent un score de 0,8 - leur appartenance à un groupe ne change rien à cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Code... à voir la semaine prochaine 'Platt scaling'!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### 3 **Calibration** : faiblesses !\n",
    "\n",
    "Cela ne change rien à la pérennisation des inégalités liées aux coûts réels (par exemple grossesses ou handicaps de productivité). \n",
    "\n",
    "Calibration ne résout pas le problème de pertinence morale dont nous avons parlé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Faiblesses des deux\n",
    "\n",
    "Ils sont incompatibles!\n",
    "\n",
    "Exemple simple..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On peut utiliser ces critères pour détecter des problèmes, mais ils ne vous indiquent pas quelle devrait être la solution. En effet, ils sont également tous deux incompatibles !\n",
    "\n",
    "\n",
    "<img src=\"incompatibility.png\" alt=\"drawing\" width=\"900\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Changez le contexte mais gardez les chiffres les mêmes...\n",
    "\n",
    "<img src=\"employment.png\" alt=\"drawing\" width=\"900\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Il n’existe pas de solution universelle au problème.** \n",
    "\n",
    "Le choix du critère dépend de la situation spécifique, et ils sont incompatibles!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Autres avertissements :\n",
    "\n",
    "Le respect des critères d'équité à l'aide de l'apprentissage automatique peut souvent conduire à des « solutions paresseuses » et à des problèmes d'équité des **sous**groupes. \n",
    "\n",
    "<img src=\"subgroup.png\" alt=\"drawing\" width=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Des solutions globales nécessitaient une compréhension détaillée des problèmes sociétaux, moraux et équitables sous-jacents liés au domaine.\n",
    "\n",
    "### Par conséquent, les ingénieurs devraient s'adresser à des experts du domaine !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Considérez** « Défaut de comparaître devant le tribunal ».\n",
    "\n",
    "**Une approche en cours de promotion** : prédire l'absence de comparution et de détention si le risque est élevé (cela peut durer longtemps - cela est dévastateur pour les familles et le soutien social, et coûteux).\n",
    "\n",
    "**Alternative** : mettre en œuvre des mesures pour atténuer les problèmes. Reconnaître que les personnes ne se présentent pas au tribunal en raison de problèmes de transport, du manque de services de garde d'enfants, d'horaires de travail ou d'un trop grand nombre de rendez-vous au tribunal.\n",
    "\n",
    "Une solution d’apprentissage automatique peut chercher à réduire les coûts mais en fin de compte faire le contraire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Résumé\n",
    "\n",
    "Les préjugés et l'équité sont un sujet important dans l'apprentissage automatique, et la loi s'adapte rapidement en réponse à la mise en œuvre généralisée de processus décisionnels automatisés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Les algorithmes débiaisés/équitables sont difficiles à concevoir, et même leur définition est difficile, nécessitant des connaissances spécialisées dans le domaine et la prise en compte de principes pratiques, éthiques et moraux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Enfin, nous sommes limités par notre accès uniquement à la distribution conjointe $p(X, Y, G...)$. L’inférence causale est une option pour explorer statistiquement ces problèmes plus profonds. Il existe un fossé sémantique difficle à combler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### La semaine prochaine:\n",
    "\n",
    "- Comment calibrer votre algorithme\n",
    "- Explicabilité de l'apprentissage automatique et perspective causale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merci et je serai heureux de répondre à vos questions !\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "def finished_lecture(condition: int = 0, thank_you_image_url: str = None):\n",
    "    # Check if the input is an integer and either 0 or 1\n",
    "    if not isinstance(condition, int) or condition not in [0, 1]:\n",
    "        raise ValueError(\"condition must be an integer and either 0 or 1\")\n",
    "\n",
    "    if condition == 1:\n",
    "        print('Merci et je serai heureux de répondre à vos questions !')\n",
    "        if thank_you_image_url:\n",
    "            display(Image(url=thank_you_image_url))\n",
    "    elif condition == 0:\n",
    "        return\n",
    "    \n",
    "    \n",
    "condition = 1\n",
    "\n",
    "finished_lecture(condition=condition)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOZiFt0GEX1EJa/xgMtoCCS",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
